---
layout: post
title: An Intuitive Derivation of Conjugate Gradient
tags: cg
---

<!-- bundle exec jekyll serve --incremental -->

New CG derivation by Rajat Dwaraknath.

Outline:

* Problem setup $A x^\* = b$
* Compute A-orthogonal basis $p_1, \dots, p_k$
* Approximate solution by A-projecting $x^\*$ onto span($p_1, \dots, p_k$)
* Choose $p_k$ such that they span Krylov sequence
* Residuals are now parallel to the $q_k$ from Lanczos
* Do Gram Schmidt in the A-inner product on $r_k$ to get $p_k$ using a two term recurrence

# Problem Setup

We wish to solve the linear system $A x^\* = b$ where $A \in \mathbb{R}^{n \times n}$ is a symmetric positive definite matrix. We use $x^\*$ to denote the exact solution.

We are working in the computational model where we have access to the matrix $A$ only through __matrix-vector__ products. That is, we have a method to compute $Av$ for any vector $v \in \mathbb{R}^n$. We also have access to the vector $b$.

Since $A$ is symmetric positive definite, it induces an inner product given by $\langle u, v\rangle_A = u^T A v$. Although we don't have access to the solution $x^\*$, we do have access to $b = A x^*$. So, we **can** compute the expression $v^T b = v^T A x^\* = \langle v, x^\* \rangle_A$ for any $v$. In other words,

__We can compute A-inner products $\langle x^\*, v \rangle_A$ with the solution for any $v$.__

# Solution Attempt
Motivated by this, we can posit a method to solve $A x^\* = b$ by working in the $A$-inner product as follows:

* Compute an $A$-orthogonal basis for $\mathbb{R}^n$ which we denote $\left\\{p_1, p_2, \dots, p_n\right\\}$ via Gram-Schmidt in the _$A$-inner product_. (Note that for this post, we don't need these basis vectors to be normalized)
* _$A$-project_ the solution $x^\*$ using this basis:

$$x^* = \sum_{i=1}^n \frac{\langle x^*, p_i \rangle_A}{\langle p_i, p_i \rangle_A} p_i \label{eq:project}$$

This also naturally leads to an approximation scheme by truncating the sum in the projection:

$$x_{\color{red}{k}} := \sum_{i=1}^{\color{red}{k}} \frac{\langle x^*, p_i \rangle_A}{\langle p_i, p_i \rangle_A} p_i$$

Therefore, the sequence of approximations $x_k$ can be interpreted as the _$A$-projection_ of the solution $x^\*$ onto to the sequence of increasing subspaces given by $\text{span}(p_1, \dots, p_k)$. We can iteratively update the approximations by noticing that:

$$\begin{aligned}x_{\color{red}{k+1}} &= \sum_{i=1}^{\color{red}{k+1}} \frac{\langle x^*, p_i \rangle_A}{\langle p_i, p_i \rangle_A} p_i\\ &= \sum_{i=1}^{\color{red}{k}}\left( \frac{\langle x^*, p_i \rangle_A}{\langle p_i, p_i \rangle_A} p_i\right) + \frac{\langle x^*, p_{k+1} \rangle_A}{\langle p_{k+1}, p_{k+1} \rangle_A} p_{k+1}\\ &= x_{\color{red}k} + \frac{\langle x^*, p_{k+1} \rangle_A}{\langle p_{k+1}, p_{k+1} \rangle_A} p_{k+1}\end{aligned}$$

Since the approximations are projections, we can also use the variational characterization of projection as finding the vector in the subspace that is closest to $x^\*$ in the _$A$-norm_:

$$x_k = \text{argmin}_{x \in \text{span}(p_1, \dots, p_k)} \Vert x - x^*\Vert_A$$

Notice that there is some freedom in the choice of $\\{p_i\\}$ in this method. We use this freedom in a specific way to arrive at the _Conjugate gradient_ method.

# Connecting to Conjugate Gradient
The conjugate gradient method does exactly the above procedure, but for a very specific choice of the orthogonal basis $\left\\{p_1, p_2, \dots, p_n\right\\}$. Specifically, it requires that these vectors span the _Krylov sequence_ of $A$ with starting vector $b$. More precisely, **CG chooses $\\{p_i\\}$ such that**

$$\text{span}(p_1, \dots, p_k) = \mathcal{K}(A, b, k) := \mathcal{K}_k \text{ for all } k$$ 

With this choice, the variational characterization of the successive approximations becomes:

$$x_k = \text{argmin}_{x \in \mathcal{K}_k} \Vert x - x^*\Vert_A$$

which is exactly the starting definition of CG!

What remains now is the find an efficient way of computing the _$A$-orthogonal_ basis $\\{p_i\\}$. It turns out that choosing the successive approximation subspaces to be the Krylov sequence allows us to compute $p_i$ using a short recurrence by connecting to the Lanczos iteration.

# The Short Recurrence for $p_i$

To compute the _$A$-orthogonal_ basis $\\{p_i\\}$, we can perform Gram-Schmidt in the _$A$-inner product_ on some vectors $v_1, \dots, v_n$ that span the Krylov sequence. However, Gram-Schmidt is pretty slow since to compute $p_k$ we need to _$A$-project_ out the components of $v_k$ along $p_1, \dots, p_{k-1}$ and each projection needs $O(nnz(A))$ time since we need to compute an $A$-inner product which requires multiplication by $A$. So the total time to compute the basis $\\{p_1, \dots , p_k\\}$ is $O(nnz(A)k^2)$. It would be nice if we only needed to project out a few components instead of all $k$ at each step. We can achieve this with a smart choice of starting vectors $v_i$.

A good choice of starting vectors $v_i$ would be one where they are already close to being $A$-orthogonal. Putting the vectors into a matrix $V_k := [v_1, \dots, v_k]$, we want the $A$-inner product of $V_k$ with its transpose to be close to a diagonal matrix. More precisely, we want $V_k^T A V_K$ to be close to a diagonal matrix. 

We have seen such a $V_k$ in the context of the Lanczos iteration. Specifically, the vectors $q_1, \dots, q_k$ generated by Lanczos span the Krylov sequence and also have the property that $Q_k^T A Q_k$ is a tridiagonal matrix. This means that 

$$q_i^T A q_j = 0 \text{ for  } |i-j| > 1 \implies q_i \perp_A q_j \text{ for  } |i-j|>1$$

In other words, $q_k \perp_A K_{k-2}$ for all $i$.

Therefore, we choose $v_k = q_k$ for all $k$. When performing _$A$-Gram Schmidt_ on $q_k$, we only need to project out the component of $q_k$ along $p_{k-1}$ since $q_k$ is already $A$-orthogonal to $p_{k-2}, \dots, p_1$. Therefore, we can write the Gram-Schmidt step as follows:

$$p_k = q_k - \frac{\langle q_k, p_{k-1}\rangle_A}{\langle p_{k-1}, p_{k-1} \rangle_A} p_{k-1}$$

This step only requires a $O(nnz(A))$ compute since we are only doing one $A$-projection. Therefore, the total time to compute $p_1, \dots , p_k$ is $O(nnz(A)k)$ which is much faster than the previous $O(nnz(A)k^2)$ time. Note that this includes the time to run the Lanczos iteration to generate $q_k$ since that also takes $O(nnz(A)k)$ time. 

Therefore, the total time to compute the approximation $x_k$ is also $O(nnz(A)k)$ since we can iteratively update the approximations to obtain $x_k$ as described before, and each of these updates also only takes $O(nnz(A))$ time. 

## Bringing in the Residuals

This version of CG might seem a bit different than the usual implementation since there is mention of the residual vectors $r_k := b - Ax_k$. We can easily bring these into the picture by noticing an important fact:

**The residuals $r_k$ are scaled versions of the vectors $q_{k+1}$ generated by Lanczos. More precisely, $r_k$ is parallel to $q_{k+1}$ for all $0 \leq k \leq n-1$.**
